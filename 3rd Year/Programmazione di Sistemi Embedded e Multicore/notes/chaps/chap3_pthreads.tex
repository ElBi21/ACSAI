\chapter{PThreads}
Con MPI, ci siamo sempre occupati di sistemi a memoria distribuita, dove i processi sono collegati da qualche mezzo (come un bus comune, o una connessione internet). Tuttavia come rappresentazione, questa è molto semplificata.
\nl
Ogni nodo (o server, o addirittura \textit{blade}) contiene una memoria DRAM e una CPU multicore. In questa CPU, per ogni core sono presenti, assieme all'ALU, anche una cache L1 privata. La CPU ha anche assegnata una cache L2 comune. Assegnato al nodo, può esserci anche una, o più, GPUs (Graphics Processing Unit) e una, o più, NICs (Network Interface Card).
\nl
Come possiamo sfruttare un cluster di servers? Sappiamo che ogni server può comunicare attraverso MPI, ma come usarlo al meglio? Dovremmo scrivere un programma di MPI per ogni core? O per ogni nodo?
\nl
Di norma, si usa creare un processo MPI per ogni nodo, e poi internamente sfruttare librerie come PThreads e CUDA (per le GPU Nvidia) per gestire rispettivamente i core / threads delle CPU e i core della GPU. Il vantaggio di usare dei threads al posto di processi separati è che i \textbf{threads condividono la memoria DRAM} della CPU.
\nl
Ridefiniamo al volo cosa intendiamo con il termine \textbf{processo} e \textbf{thread}, così da poter distinguere la differenza tra un processo e un thread:

\begin{definition}{Processo}
    Un \textbf{processo} è un'\textbf{istanza di computazione} di un programma che può essere in esecuzione, in stato di attesa o sospeso.
\end{definition}

\begin{definition}{Thread}
    Un \textbf{thread} è l'\textbf{istanza di computazione più piccola} e \textbf{indipendente} che può essere eseguita su un computer.
\end{definition}

Sui sistemi UNIX, un processo si crea tramite la chiamata di sistema \verb|fork()|. Facendo così, il processo padre viene duplicato e riassegnato al processo figlio. Quando viene creato il processo, viene copiato il codice del processo, la sua memoria, etc... Una volta copiata la memoria del processo, questa viene poi sovrascritta con il codice
\nl
Sempre sui processi UNIX, si usa la libreria PThreads, che sta per POSIX Threads. Questa libreria può essere utilizzata tramite C, includendola proprio come veniva fatto per MPI.
\nl
Con MPI dovevamo usare un wrapper specifico di \verb|gcc|, ma con PThreads non è necessario: i threads infatti vengono creati automaticamente una volta lanciato il programma. La funzione per creare un thread è la seguente:

\begin{codedefine}
    \begin{lstlisting}[language = C, numbers = none]
int pthread_create (
    pthread_t* thread_p,
    const pthread_attr_t* attr_p,
    void* (*start_routine) (void*),
    void* arg_p
)
    \end{lstlisting}
    \tcblower
    Parametri di \verb|pthread_create()|:
    \begin{itemize}
        \item \verb|thread_p|: è un handle che rappresenta lo stato del thread. Tale handle è opaco, quindi non va modificato, e il puntatore va sempre allocato prima della chiamata della funzione. PThreads garantisce che tale handle contenga abbastanza informazioni per identificare il thread;
        \item \verb|attr_p|: è un set di attributi, che non verranno coperti in queste note;
        \item \verb|(*start_routine)|: è un puntatore a funzione, che definisce da dove il thread inizierà ad eseguire il codice;
        \item \verb|arg_p|: è un puntatore ai parametri che la funzione \verb|start_routine| richiede.
    \end{itemize}
\end{codedefine}

Un puntatore a funzione è semplicemente un indirizzo di memoria che indica il luogo in memoria dove è immagazzinata una funzione. Il nome di una funzione è utilizzabile per ottenere il puntatore della funzione stessa. Ad esempio:

\begin{codeblock}{FunctionPointer.c}
    \begin{lstlisting}[language = C]
void func(int a) {
    printf("Hello world");
}

void main() {
    // Riprendere dalle slides...
}\end{lstlisting}
\end{codeblock}

Le variabili globali sono visibili da tutti i threads, tuttavia è meglio evitarne l'uso a meno che non sia assolutamente necessario.
\nl
Per attendere la fine dell'esecuzione di un thread, si usa la funzione \verb|pthread_join()|. Tale funzione richiede in input l'handle del thread, e aspetterà finché il thread non terminerà la sua esecuzione. 

% C'è la possibilità, tramite alcune syscalls, di far sì che un thread venga eseguito specificamente su un core. Questo è comodo quando si ha un programma che crea tanti threads quasi quanto è il numero di core sulla CPU, cosicché il sistema operativo non ridistribuisca magari, dopo un tot di tempo, i threads su un unico core.

\section{Concorrenza}

Supponiamo di voler creare, similarmente a come è stato fatto per MPI, un programma che calcoli il vlaore di $\pi$ tramite approssimazione. Proviamo dunque a scrivere una versione del codice con PThreads:

\begin{codeblock}{PthreadPi.c}
    \begin{lstlisting}[language = C]
// Put code...\end{lstlisting}
\end{codeblock}

Proviamo a vedere il valore delle varie approssimazioni in base al numero di threads $t$ con cui eseguiamo il programma:

\begin{center}
    \begin{tabular}{|c||c|c|c|c|}
        \hline
        \multirow{2}{*}{Threads} & \multicolumn{4}{c|}{$n$} \\
        \cline{2-5}\rule{0pt}{2.5ex} 
        & $10^5$ & $10^6$ & $10^7$ & $10^8$ \\
        \hline\hline
        $\pi$ & $3,14159$ & $3,141593$ & $3,1415927$ & $3,14159265$ \\
        \hline
        $t \eq 1$ & $3,14158$ & $3,141592$ & $3,1415926$ & $3,14159264$ \\
        \hline
        $t \eq 2$ & $3,14158$ & $3,141480$ & $3,1413692$ & $3,14164686$ \\
        \hline
    \end{tabular}
\end{center}

Come possiamo notare, più il numero di threads aumenta, più la stima si allontana dal valore reale. Questo però è controintuitivo, in quanto ci aspetteremmo che con più threads la stima dovrebbe essere più accurata. Quindi, come mai succede questo?
\nl
Il motivo sta nella riga di codice \verb|sum += factor/(2 * i + 1)|, più nello specifico, nel fatto che la variabile \verb|sum| è condivisa da tutti i threads. Quando si fa l'operazione \verb|+=| non stiamo facendo solo una somma, ma stiamo eseguendo più istruzione.

\subsection{Busy Waiting}

\subsection{Mutex}

Un modo alternativo per far sì che l'accesso a una variabile sia controllato è usare una \textbf{mutex} (mutual exclusive). Le mutex esistono per ovviare al problema del busy waiting, cioè che la CPU continua a non eseguire nulla mentre controlla il valore di una variabile.

\begin{definition}{Mutex}
    Una variabile \textbf{mutex} (mutually exclusive) è una variabile che \textbf{controlla l'accesso} a una sezione critica, e che permette a un solo thread alla volta di accedervi.
\end{definition}

PThread implementa il concetto di mutex attraverso un tipo speciale: \verb|pthread_mutex_t|. Le mutex sono implementate attraverso istruzioni di Assembly particolari, come ad esempio le \texttt{test/set}. Le istruzioni \texttt{test/set} sono istruzioni \textbf{atomiche} che riescono a controllare una variabile e impostarla a 1 quando essa è 0. 

\section{Semafori}

temp

\section{Barriere e Variabili di Confizioni}

temp

\section{Read-Write Locks}

% Avendo una struct tipo
%
%   struct ciao {
%       int data,
%       ciao* ptr
%   }
%
% La notazione ciao -> data ritorna il valore contenuto in data

Quando si usa una struttura di dati condivisa, è importante controllare l'accesso a tale struttura. Consideriamo ad esempio una linked list \textbf{ordinata} senza doppi elementi (cioè una lista di elementi dove ogni elemento contiene un valore e un puntatore al prossimo elemento) dove le possibili operationi saranno \verb|Member| (che controlla se un elemento è nella lista), \verb|Insert| (che inserisce un elemento nella lista) e \verb|Delete| (che rimuove un elemento).
\nl
Come possiamo far sì che questa lista sia utilizzabile in parallelo? Perché chiaramente scrivere o cancellare elementi da una lista necessita di una sezione critica. Finché più threads leggono solo la lista non è un problema, ma quando si fanno operazioni di scrittura allora si creano vari problemi.
\nl
La soluzione più semplice è di usare un lock sulla lista, cosicché se un thread deve usare la lista dovrà prima ottenere il relativo lock, per poi rilasciarlo una volta finito. Questo però serializza la lista, sia in lettura che in scrittura, e soprattutto se la maggior parte delle operazioni fosse di lettura (quindi con \verb|Member|) allora perderemmo una chance di parallelizzare la lista. Nel caso in cui ad esempio si facessero più operazioni di scrittura che di lettura allora non sarebbe così tanto problematico. Chiaramente ci sarebbe un certo tempo necessario per acquisire il lock e per liberarlo, che non è negligibile.
\nl
Una seconda idea è quella di usare un lock per ogni nodo della lista, che nonostante sia più complesso da implementare, garantirebbe a tutti di poter usare la lista. Questo approccio però è parecchio lento, siccome ogni volta che bisogna accedere a un nodo serve acquisirne il relativo lock. Inoltre, a livello di memoria si va ad occupare parecchio spazio per immagazzinare ogni lock.
\nl
L'idea ottimale sarebbe quella di usare un lock solo in fase di scrittura, e non in fase di lettura. Questo tipo di lock esiste e si chiama \textbf{Read-Write Lock}.

\begin{definition}{Read-Write Lock}
    Un \textbf{Read-Write Lock} è una \textbf{mutex} che permette di \textbf{differenziare} il \textbf{tipo di lock} in base all'operazione da fare. Pthreads implementa tale lock usando due funzioni: una per il \textbf{lock in lettura} e una per il \textbf{lock in scrittura}.
\end{definition}

Come ogni lock, c'è una funzione per inizializzare il lock e una per distruggerla:

\begin{codedefine}
    \begin{lstlisting}[language = C, numbers = none]
int pthread_rwlock_init (
    pthread_rwlock_t* rwlock,
    pthread_rwlockattr_t* attr
);


int pthread_rwlock_destroy (
    pthread_rwlock_t* rwlock
);\end{lstlisting}
\tcblower

\end{codedefine}