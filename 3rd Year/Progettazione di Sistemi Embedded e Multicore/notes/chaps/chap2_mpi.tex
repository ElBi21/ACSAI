\chapter{Message Passing Interface (MPI)}

\textbf{MPI} (acronimo di \textbf{M}essage \textbf{P}assing \textbf{I}nterface) è una libreria usata per \textbf{programmare sistemi a memoria distribuita}, e delle varie librerie menzionate nell'introduzione è l'unica pensata per sistemi a memoria distribuita. Questo vuol dire che la memoria e il core usato per ogni thread o processo sono \textbf{unici}. Tale core e memoria possono essere collegati attraverso vari metodi: un bus, la rete, etc...
\\\\
MPI fa uso del paradigma \textbf{Single Program Multiple Data} (\textbf{SPMD}), quindi ci sarà un \textbf{unico programma} che verrà compilato e poi eseguito da vari processi o threads. Per determinare cosa ogni processo o thread deve fare, si usa semplicemente un'istruzione di \textbf{branching}, come l'\texttt{if-else} o lo \texttt{switch}.
\\\\
Siccome la memoria non è condivisa tra i vari processi, l'unico modo per passarsi dei dati è attraverso l'invio di \textbf{messaggi} (da qui il nome della libreria). Per esempio, abbiamo visto come fare un semplice "Hello world" in C in modo sequenziale, ma possiamo anche renderlo parallelo tramite MPI. Ad esempio:

\begin{codeblock}{HelloWorld.c}
    \begin{lstlisting}[language = C]
#include <stdio.h>

int main() {
    printf("Hello world");
    return 0;
}\end{lstlisting}
\end{codeblock}

Per rendere questo Hello World un programma parallelo tramite MPI, serve includere la libreria \texttt{mpi.h} e usare alcune funzioni della libreria. Vediamo intanto come potremmo scrivere il programma:

\begin{codeblock}{ParallelHelloWorld.c}
    \begin{lstlisting}[language = C]
#include <stdio.h>
#include <mpi.h>

int main(void) {
    // Per usare MPI, serve usare una funzione chiamata MPI_INIT;
    int r = MPI_Init(NULL, NULL);

    if(r != MPI_SUCCESS) {
        printf("C'è stato un errore con il programma");
        MPI_Abort(MPI_COMM_WORLD, r);
    }

    printf("Hello world");

    // Per terminare l'esecuzione di tutti i threads si usa MPI_Finalize
    MPI_Finalize();
    return 0;
}\end{lstlisting}
\end{codeblock}

Nel precedente codice sono state usate alcune funzioni e alcuni valori di MPI, che possiamo notare grazie al prefix "\texttt{MPI\_}", \textbf{comune a tutte le definizioni}, siano esse di funzioni, variabili o costanti, \textbf{della libreria}:
\begin{itemize}
    \item \verb|MPI_Init()|: \textbf{inizializza} un programma su più processi o threads, e restituisce come output un \texttt{int}, che identifica se è stato possibile inizializzare con successo la libreria di MPI o meno (ovverosia restituisce 0 se la libreria è stata inizializzata con successo, un altro numero altrimenti);
    \item \verb|MPI_SUCCESS|: è il segnale con cui è possibile comparare l'output di \verb|MPI_Init| per controllare se MPI è stato inizializzato correttamente o meno;
    \item \verb|MPI_Abort(MPI_COMM_WORLD, <mpi_boot_result>)|: \textbf{abortisce} l'esecuzione di MPI, ad esempio nel caso in cui l'inizializzazione non sia stata eseguita con successo;
    \item \verb|MPI_Finalize()|: \textbf{interrompe} l'esecuzione di MPI a fine programma.
\end{itemize}

Per compilare ed eseguire un programma con MPI si usa \texttt{mpicc}, che è un wrapper del compilatore \texttt{gcc} di C. Un comando che viene usato per compilare un programma che usa MPI può essere il seguente:

\begin{terminal}
    \begin{lstlisting}[style = notexterm]
$ mpicc <file>.c -o <output>
$ mpicc -g -Wall <file>.c -o <output>    # Fa stampare i warning in console\end{lstlisting}
\end{terminal}

Il compilatore ha molte flags che possono essere usate, così da personalizzare il processo di compilazione. Nel secondo comando si può notare l'uso di due flags che possono risultare comode in fase di debug:
\begin{itemize}
    \item \verb|-Wall|: fa stampare in console \textbf{tutti i warnings} del compilatore;
    \item \verb|-g|: fa stampare in console varie \textbf{informazioni di debug}.
\end{itemize}

Questo è per quanto riguarda la compilazione, ma per eseguire il programma invece? Dovremo usare \texttt{mpirun}, attraverso il seguente comando:
\begin{terminal}
    \begin{lstlisting}[style = notexterm]
$ mpirun -n <numero_core_fisici> <programma>
$ mpirun --oversubscribe -n <numero_core> <programma>
        # Permette di usare più core di quelli fisici\end{lstlisting}
\end{terminal}

Normalmente MPI esegue il codice solo sui core fisici di una CPU, tuttavia è possibile far sì che questa limitazione non venga considerata. La flag \verb|--oversubscribe| permette di lanciare il programma su $n$ processi, dove $n \geq \text{numero di core fisici}$.
\\\\
In un programma complesso, è spesso utile sapere quale core esegue quale parte di programma, magari anche per assegnare dei compiti diversi ad ogni core. In questi casi, possiamo differenziare i processi in base al loro \textbf{rank}.

\begin{definition}{Rank}
    Il \textbf{rank} di un processo appartenente a un programma di MPI è un \textbf{indice incrementale}, nell'intervallo $[0, \; 1, \; 2, \; \dots , \; p)$, che viene assegnato ad ogni processo.
\end{definition}

% ...

\section{Misurazione delle Prestazioni}

Una volta che sviluppiamo un programma con MPI, è importante misurarne le prestazioni. Esistono vari modi per controllare l'efficienza di un programma, e qui ne vedremo alcuni.
\\\\
Un modo molto semplice per capire l'efficienza di un programma consiste nel misurare il tempo di esecuzione, e possiamo farlo tramite la funzione \verb|MPI_Wtime()|, la quale ritorna un timestamp che misura il tempo dal momento in cui il programma viene eseguito. Per poter ottenere il tempo di esecuzione, bisognerà sottrarre dal tempo a fine esecuzione il tempo di quanto la funzione viene chiamata.
\\\\
In un programma parallelo, ogni processo calcola il suo tempo, ma può essere che alcuni processi impieghino meno tempo, soprattutto se ogni processo esegue operazioni diverse. Per questo motivo, l'efficienza si calcola considerando il tempo maggiore. Per ovviare a questo problema, tutti i processi possono mandare a un solo processo i loro tempi, e tramite una chiamata di \verb|MPI_Reduce()| si può ritornare solo il tempo maggiore.

\begin{codeblock}
    \begin{lstlisting}[language = C, numbers = none, columns=fullflexible]
double local_start, local_finish, local_elapsed, elapsed;
local_start = MPI_Wtime();

// Codice...

local_finish = MPI_Wtime();
local_elapsed = local_finish - local_start;

MPI_Reduce(&local_elapsed, &elapsed, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);

if (rank == 0) {
    printf("Tempo totale di esecuzione: %e\n", elapsed);
}\end{lstlisting}
\end{codeblock}

Tuttavia, un altro problema si presenta: \textbf{non tutti i processi possono iniziare allo stesso tempo}. E questo chiaramente è un problema, soprattutto se il processo che inizia in ritardo inizia parecchio in ritardo. Un modo per far sì che tutti i processi inizino assieme è tramite l'uso di una funzione chiamata \verb|MPI_Barrier()|. Tale funzione è una \textbf{funzione collettiva}, che si propaga a tutti i processi, e che li blocca fino al momento in cui tutti non raggiungono tale barriera. Possiamo considerare questa funzione come una sorta di checkpoint, che \textbf{posta all'inizio del programma} fa sì che tutti i processi inizino ad eseguire il loro codice "assieme".
\\\\
Tuttavia, se anche questa collettiva generasse ritardi venendo propagata, non avremmo più un inizio collettivo. Riuscire a sincronizzare tutti i processi assieme è complicato, e richiederebbe l'uso di clock interni per ogni core. Tuttavia, in casi in cui non sia fondamentale avere grande precisione, \verb|MPI_Barrier()| \textbf{garantisce} una \textbf{buona approssimazione}.
\\\\
Ma è abbastanza eseguire l'applicazione solo una volta per avere una buona misurazione? Generalmente no, e ci sono vari motivi: sappiamo che il sistema operativo, di tanto in tanto, può effettuare dei cambi di contesto, per cui uno o più core vengono usati per eseguire delle operazioni dell'OS. Questo chiaramente può aumentare i tempi per alcune esecuzioni. In altri casi invece potrebbe essere che lo scheduler della memoria liberi la cache prima del necessario, etc...
\\\\
Tali motivi di ritardo vengono chiamati \textbf{rumore} (o \textbf{noise}). In genere, per misurare l'efficienza di un programma, \textbf{conviene sempre basarsi su tutte le esecuzioni di un programma}, considerando media, mediana, il tipo di distribuzione, eventuali intervalli di confidenza, etc... Il tempo di esecuzione minimo, da solo, non indica molto, poiché per vari motivi di interferenza può non sempre (e in realtà non accade quasi mai) indicare la vera efficienza del programma.

\subsection{Speed-Up ed Efficienza}

Generalmente, potremmo aspettarci che se un processo viene eseguito da quanti più core possibili, allora potremmo ottenere tempi sempre più bassi. Benché in molti casi questo sia vero, lo speed-up dell'esecuzione parallela non è sempre esistente aumentando il numero di processi e di core. Infatti, consideriamo il seguente esempio:

% tabella con i tempi

Se notiamo la prima colonna, il tempo di esecuzione con 8 e 16 core è identico. Questo perché, oltre a un certo punto, anche dividendo le operazioni su più core, raggiungeremmo un tempo minimo di inizializzazione che non è ottimizzabile. Dunque, lo speed-up terminerebbe. Ma cosa intendiamo effettivamente con speed-up?

\begin{definition}{Speed-up}
    Definiamo le seguenti variabili:
    \begin{itemize}
        \item $T_s(n)$: il \textbf{tempo di esecuzione} di un problema di dimensione $n$ in \textbf{seriale};
        \item $T_p(n, \; p)$: il \textbf{tempo di esecuzione} di un problema di dimensione $n$ eseguito in \textbf{parallelo} su $p$ core;
    \end{itemize}

    Lo \textbf{speed-up} di un'applicazione in parallelo su $p$ core viene dunque definito come il rapporto tra il tempo di esecuzione in seriale e il tempo di esecuzione in parallelo con $p$ core:
    \[ S(n, \; p) \eq \frac{T_s(n)}{T_p(n, \; p)} \]

    Se $S(n, \; p) \eq p$, allora lo speed-up viene definito come \textbf{speed-up lineare}.
\end{definition}

C'è un dettaglio importante di questa precedente definizione: il tempo di esecuzione in sequenziale \textbf{non è uguale} al tempo di esecuzione in parallelo con $p \eq 1$, anzi, tendenzialmente $t_p(n, \; 1) \geq t_s(n)$. A causa di questo, possiamo definire due implementazioni diverse della precedente formula:

\begin{center}
    \begin{tabular}{c c c}
        $S(n, \; p) \eq \frac{T_s(n)}{T_p(n, \; p)}$ & \quad \quad \quad \quad & $S(n, \; p) \eq \frac{T_p(n, \; 1)}{T_p(n, \; p)}$ \\ & & \\
        \textbf{Speed-up} & & \textbf{Scalabilità}
    \end{tabular}
\end{center}

Grazie alla definizione di speed-up, possiamo definire anche il concetto di \textbf{efficienza}:

\begin{definition}{Efficienza}
    L'\textbf{efficienza} di un programma viene calcolata come il rapporto tra lo speed-up di un programma e il numero di core su cui viene eseguito:
    \[ E(n, \; p) \eq \frac{S(n, \; p)}{p} \eq \frac{T_s(n)}{p \cdot T_p(n, \; p)} \]
\end{definition}

\subsection{Scalabilità Forte e Scalabilità Debole}

Esistono due tipi di scalabilità, in base ai risultati che si ottengono dalle misurazioni dell'efficienza di un programma: \textbf{scalabilità forte} e \textbf{scalabilità debole}.

\begin{itemize}
    \item Per \textbf{scalabilità forte} si intende quando, dato un problema di grandezza $n$, se si incrementa il numero di processi $p$, allora l'efficienza rimane alta;
    \item Per \textbf{scalabilità debole} si intende quando, dato un problema di grandezza $n$ e un numero di processi $p$, se incrementando ugualmente $n$ e $p$, allora l'efficienza rimane alta.
\end{itemize}

\begin{center}
    \begin{tabular}{|c||c|c|c|c|c|}
        \hline
        \multirow{2}{*}{ \texttt{comm\_size} } & \multicolumn{5}{c|}{Ordine della matrice} \\
         & $1024$ & $2048$ & $4096$ & $8192$ & $16384$ \\
        \hline\hline
    \end{tabular}
\end{center}

% inserisci seconda tabella

\subsection{Leggi di Amdhal e Gustafson}

Quando dobbiamo rendere un programma parallelo, sappiamo che possiamo parallelizzare solo alcune operazioni: ad esempio, leggere dal disco dei dati, richiedere dei dati in input all'utente o mandare dati attraverso un comunicatore di MPI. Avremo dunque una parte \textbf{sempre sequenziale} e una parte \textbf{parallelizzabile}. La frazione del programma sempre sequenziale viene indicata con $\alpha$. C'è una legge, chiamata \textbf{legge di Amdhal}, che definisce lo speed-up possibile di un'applicazione.

\begin{definition}{Legge di Amdhal}
    Per la \textbf{legge di Amdhal}, lo speed-up di un'applicazione, se resa parallela, è limitato dalla \textbf{frazione di codice sequenziale} $\alpha$:
    \[ T_p(n, \; p) \eq (1 - \alpha) \cdot T_s(n) + \alpha \cdot \frac{T_s(n)}{p} \]

    Lo speed-up calcolabile sarà dunque uguale a
    \[ S(n, p) \eq \frac{T_s(n)}{(1 - \alpha) \cdot T_s(n) + \alpha \cdot \frac{T_s(n)}{p}} \]
\end{definition}

Generalmente, se portassimo il numero di core $p$ all'infinito, raggiungeremmo il seguente valore:
\[ \lim_{x \to +\infty} S(n, \; p) \eq \frac{1}{1 - \alpha} \]

\noindent La legge di Amdhal tuttavia ha dei problemi: intanto non tiene conto della scalabilità debole (poiché la legge di Amdhal considera un $n$ costante) % continua...

\begin{definition}{Legge di Gustafson}
    La legge di Gustafson definisce ciò che si chiama \textbf{speed-up scalabile}, e che prende in assunzione che, se si aumenta il numero di processi $p$ per una costante $a$, allora anche la dimensione del problema $n$ aumenta di $a$.
    \[ S(n, \; p) \eq (1 - \alpha) + \alpha \cdot p \]
\end{definition}

\section{Esempi con MPI}

% inserire il parallel sorting

Un altro esempio di codice parallelizzabile è la \textbf{somma tra vettori}. Per questo esempio, abbiamo dei vettori composti da $n$ elementi, e abbiamo $p$ core disponibili per fare la somma delle componenti dei vettori. Intuitivamente, il metodo più semplice per parallelizzare tale esempio è assegnare a ogni core un numero $\frac{n}{p}$ di elementi da ogni vettore e calcolare dunque la somma tra le varie parti autonomamente.
\\\\
MPI ha una funzione collettiva che ci permette di mandare ad ogni processo una parte di un elemento, ovverosia \verb|MPI_Scatter()|. Tale funzione è definita come segue:

\begin{lstlisting}[language = C, numbers = none]
int MPI_Scatter(
    void*          send_buff_p,
    int            send_count,
    MPI_Datatype   send_type,
    void*          recv_buff_p,
    int            recv_count,
    MPI_Datatype   recv_type,
    int            src_proc,
    MPI_Comm       comm
)\end{lstlisting}

\section{Tipi Derivati}

Sappiamo che in C è possibile creare \verb|struct|s, ovverosia composizioni di vari tipi. Tuttavia, nel caso in cui volessimo usare una \verb|struct| custom, come possiamo dire ad MPI come funziona la nostra \verb|struct|? Si possono definire dei \textbf{tipi di MPI derivati}. Ad esempio, nell'esercizio del trapezio, noi per inviare dei dati abbiamo impiegato 3 invii e 3 ricezioni. Possiamo fare di meglio utilizzando proprio i tipi derviati.

\begin{definition}{Tipi Derivati}
    Un \textbf{tipo derivato} di MPI consiste in una \textbf{sequenza} di \textbf{tipi primitivi} di MPI, che indica anche lo spazio che ogni tipo primitivo occupa in memoria.
\end{definition}

Possiamo creare nuovi tipi attraverso la funzione \verb|MPI_Type_create_struct()|, che è definita come segue:

\begin{lstlisting}[language = C, numbers = none]
int MPI_Type_create_struct (
    int             count,
    int             array_of_blocklengths[],
    MPI_Aint        array_of_displacements[],
    MPI_Datatype    array_of_types[],
    MPI_Datatype*   new_type_p
)\end{lstlisting}

Nel seguente codice, vedremo come trasformare una \verb|struct| di C in un tipo derivato:

\begin{codeblock}
    \begin{lstlisting}[language = C, numbers = none, columns=fullflexible]
struct my_struct {
    double a;
    double b;
    int c;
}

// Trasformiamo my_struct in un tipo derivato
MPI_Aint a_addr, b_addr, c_addr;

MPI_Get_address(&a, %a_addr);
array_of_displacements[0] = 0;
MPI_Get_address(&b, %b_addr);
array_of_displacements[0] = b_addr - a_addr;
MPI_Get_address(&c, %c_addr);
array_of_displacements[0] = c_addr - b_addr;

// Facciamo il commit del tipo derivato
MPI_Type_commit()
    \end{lstlisting}
\end{codeblock}